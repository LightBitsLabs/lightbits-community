# lbprox Guide

- [lbprox Guide](#lbprox-guide)
  - [`lbprox` Installation](#lbprox-installation)
    - [Install `lbprox` Using docker compose (Recommended)](#install-lbprox-using-docker-compose-recommended)
  - [Setup workdir for lbprox cli](#setup-workdir-for-lbprox-cli)
  - [Initial Proxmox Nodes Setup](#initial-proxmox-nodes-setup)
    - [Storage setup](#storage-setup)
    - [Open VFs](#open-vfs)
    - [Create Simple Zone for hypervisor internal data network](#create-simple-zone-for-hypervisor-internal-data-network)
  - [Day-To-Day Operations](#day-to-day-operations)
    - [Flavors and Descriptors](#flavors-and-descriptors)
    - [Install Lightbits On VMs](#install-lightbits-on-vms)
    - [Query cluster resources](#query-cluster-resources)
    - [Delete Everything](#delete-everything)
    - [Install `lbprox` python package in virtual-environment](#install-lbprox-python-package-in-virtual-environment)
  - [TODO](#todo)

## `lbprox` Installation

### Install `lbprox` Using docker compose (Recommended)

Install `docker` and `docker compose` followng these [instructions](https://docs.docker.com/engine/install/) (choose your OS accordingly).

First, clone the project:

```bash
git clone https://github.com/LightBitsLabs/lightbits-community.git
cd lightbits-community/proxmox/lbprox
```

Next, create a `.env` file that would contain some of the env-vars we need for compose.

Issue following command from `lightbits-community/proxmox/lbprox` to generate `.env` file and populate it with relevant fields:

```bash
cat <<EOF > .env
LBPROX_IMG=lbdocker:5000/lbprox:v0.1.0
UID=$(id -u)
GID=$(id -g)
UNAME=$(whoami)
DOCKER_GID=$(getent group docker | cut -d: -f3)
EOF
```

Create a script to invoke `lbprox` using docker compose easily:

```bash
mkdir -p ~/.local/bin
cat <<EOF > ~/.local/bin/lbprox
#!/usr/bin/env bash
set -e -u
docker compose -f `pwd`/docker-compose.yml run -it --rm lbprox lbprox "\$@"
EOF
chmod +x ~/.local/bin/lbprox
```

Now you can run the following command to get the help output:

```bash
lbprox --help
```

## Setup workdir for lbprox cli

Some of the files generated by `lbprox` are stored on the base directory at `~/.local/lbprox`

```bash
mkdir -p ~/.local/lbprox
```

Initially you would need to create a config file stating the servers in proxmox cluster that are present.

For example place the following file: `~/.local/lbprox/lbprox.yml`:

```yaml
username: root
password: light
light_app_path: ""
last_active: ""
nodes:
- hostname: rack16-server01
- hostname: rack16-server02
```

## Initial Proxmox Nodes Setup

### Storage setup

First we need to setup local-storage as Directory to hold our VMs and NVMe emulated SSDs.
In the following example we will utilize `/dev/nvme0n1` as a local attached drive that we will
Create a storage on it.

> NOTE:
>
> Today we require that all nodes will have the same block-device (like `/dev/nvme0n1`) in order to create
> a unified storage on each server in the cluster.
> The following command will fail in case not all servers have the same block-device
> We should make this more generic in the future.

Wipe the disk and create a new partition with ext4 on it and will add it as a storage named `lb-local-storage`:

```bash
lbprox image-store create rack16-server01 -s lb-local-storage -d /dev/nvme0n1
```

Now we need to pull the lightbits basic image to install:

> NOTE:
>
> By default each node's `lb-local-storage` will be populated with this image.
> You can specify specific target nodes using the multiple --node/-n flag:

```bash
lbprox os-images create -u https://pulp03.lab.lightbitslabs.com/pulp/content/rocky-9-target/qcow2/latest/rocky-9-target.qcow2
```

```bash
lbprox os-images create -u https://pulp03.lab.lightbitslabs.com/pulp/content/ubuntu-24.04-initiator/qcow2/latest/ubuntu-24.04-initiator.qcow2
```

List the os-images with the following command (filtered by iso):

```bash
lbprox os-images list -c iso
```

In order to delete an image from one or all the nodes you can run the following command:

```bash
lbprox os-images delete --volid lb-local-storage:iso/ubuntu-24.04-initiator.img
```

If we want to reverse this process and clean everything up we should run the following:

```bash
lbprox image-store delete -s lb-local-storage
```

### Open VFs

We would want to open virtual functions on the nics so we can add them as passthrough to the VMs.
On the Proxmox server run the following command to open all VFs on available Mellanox devices:

```bash
lbprox setup-data-network
```

### Create Simple Zone for hypervisor internal data network

In order to provide isolation, and DHCP inside each Proxmox node we can utilize the simple Zone network.

> NOTE:
>
> install `dnsmasq` on each proxmox node to get this functionality using:
>
> ```bash
> apt install dnsmasq -y
> NOTE: Now there are 2 dnsmasq services - dnsmasq and dnsmasq@<network_name> - we should disable the default one. (`network_name` is data0 in our example)
> systemctl disable dnsmasq.service
> ```

Following command will deploy on all nodes the same DHCP server with the same IPAM configuration that will
allow allocating dynamic IPs in the range: `10.101.1.10-10.101.1.200`

```bash
lbprox --debug data-network create --zone-name=data0
```

> NOTE:
>
> This is a cluster level command that will configure this for every node that exists in the cluster
> This command should be run every time we add a new node to the Proxmox cluster.

In order to delete this network and all other resources related you can run the following command:

```bash
lbprox data-network delete --zone-name=data0
```

## Day-To-Day Operations

### Flavors and Descriptors

A machine flavor is a well defined set of machine properties like CPU memory ssds and network configuration.

We provide a set of predefined flavors and one can add flavors as he wish:

By running the following command:

Flavor name should strive to be in the following format:

`<role>-<size>-<numa-count>-<extra_info>`

Example roles:

- target
- initiator
- observability

Example sizes:

- tiny
- small
- large

```bash
lbprox allocation list flavors
+-------+---------------------------------------+-------+-------------+------+--------+------+
| index | name                                  | cores | base_memory | ssds | ifaces | numa |
+-------+---------------------------------------+-------+-------------+------+--------+------+
| 1     | target-small-1numa-large-emulated-ssd | 9     | 20GB        | 6    | 2      | 1    |
| 2     | target-small-1numa-small-emulated-ssd | 9     | 12GB        | 4    | 2      | 1    |
| 3     | target-tiny-1numa-emulated-ssd        | 5     | 12GB        | 4    | 2      | 1    |
| 4     | target-small-1numa-4-passthrough-ssd  | 9     | 24GB        | 4    | 2      | 1    |
| 5     | target-small-1numa-6-passthrough-ssd  | 9     | 30GB        | 6    | 2      | 1    |
| 6     | initiator-small                       | 1     | 1GB         | 0    | 2      | 1    |
| 7     | observability-small                   | 2     | 4GB         | 0    | 1      | 1    |
| 8     | dms                                   | 4     | 12GB        | 0    | 1      | 1    |
+-------+---------------------------------------+-------+-------------+------+--------+------+
```

Or create entire cluster:

first list the allocation-descriptors that exists:

```bash
lbprox allocations list descriptors
+-------+----------------------------------------------------------------------+---------------+-----------------------------------------------------------+
| index | name                                                                 | machine count | description                                               |
+-------+----------------------------------------------------------------------+---------------+-----------------------------------------------------------+
| 1     | lightbits_cluster_12x_target_small_1numa_emulated_ssd_1x_client      | 13            | 12 target cluster with 1 client                           |
| 2     | lightbits_cluster_3x_target_small_1numa_large_emulated_ssd_1x_client | 4             | 3 targets cluster with 6 large disks and 1 client         |
| 3     | dms                                                                  | 1             | all in one DMS server                                     |
| 4     | lightbits_cluster_12x_target_tiny_1numa_emulated_ssd_1x_client       | 13            | 12 tiny target cluster with 1 client                      |
| 5     | lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client        | 4             | 3 tiny targets cluster with 1 client                      |
| 6     | lightbits_cluster_3x_target_small_1numa_small_emulated_ssd_1x_client | 4             | 3 targets cluster with small 4 ssds and 1 client          |
| 7     | lightbits_cluster_3x_target_small_1numa_4_phys_ssd_1x_client         | 4             | 3 small targets cluster with 4 physical ssds and 1 client |
| 8     | lightbits_cluster_3x_target_small_1numa_6_phys_ssd_1x_client         | 4             | 3 small targets cluster with 6 physical ssds and 1 client |
| 9     | observability                                                        | 1             | all in 1 observability VM                                 |
+-------+----------------------------------------------------------------------+---------------+-----------------------------------------------------------+
```

Running the following command will deploy the VMs described by selected descriptor - `lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client`

```bash
lbprox allocations create <PROXMOX_NODE_NAME> -n lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client
```

> **NOTE:**
>
> Hostname must match the Proxmox `node-name` that appears under Datacenter in Proxmox. Fox example, in the following screenshot the hostname must match `pxmx-cl6`.
>
> Don't use IP address of the node.
>
> ![alt text](image.png)

> **NOTE:**
>
> If you want to create a VM with a none emulated SSDs you should first verify that you have enough
> free unattached SSDs on your node and use the `--no-emulated --disk-count=4` option to specify we
> want to use passthrough SSDs

### Install Lightbits On VMs

The following command will install lightbits software (`v3.10.1`) on the pre-allocated VMs.
It will match the tag `cname.c00` and create inventory for every server that is part of this cluster.

The inventory files will be created at: `~/.local/lbprox/inventories/<cluster_id>`

By default the command will install lightbits by running lb-ansible from this location.

```bash
lbprox allocations deploy lightbits \
  -a <allocation_id> \
  --base-url=https://pulp02/pulp/content/releases/lightbits/3.10.1/rhel/9/67/ \
  -p <profile-name> \
  --ec-enabled \
  --initial-device-count=4
```

In order to deploy the initiator you can run the following command:

```bash
lbprox allocations deploy nvme-initiator \
  -a <allocation_id> \
  --base-url=https://pulp02/pulp/content/releases/lightbits/3.10.1/rhel/9/67/
```

### Query cluster resources

```bash
lbprox list-cluster-resources <qemu|node|storage> | jq
```

### Delete Everything

Delete VMs one by one:

```bash
lbprox allocations delete -a <allocation_id>
```

### Install `lbprox` python package in virtual-environment

<details>
<summary>Expand...</summary>

`lbprox` is a python package that is recommended to be installed in virtual environment.

In order to setup virtual environment. The following commands will install venv if not
already installed on your system, and will create a virtual-environment under `.venv`
folder. It will also activate this virtual environment, and all installations will
be placed at this env.

```bash
sudo apt install python3-venv
cd lbprox
python3 -m venv .venv
source .venv/bin/activate
```

Now we want to install the `lbprox` package with all it's dependencies. Following
command will install this package:

> `NOTE:`
>
> `-e` state that we install this package in edit mode (for dev purposes) so if we
> change the source code it will apply to the installed command line immediately without
> needing to reinstall the package.

```bash
pip install -e .
```

Now you can run the following command to get the help output:

```bash
lbprox --help
```
In order to deactivate from this environment just run the following command:

```bash
deactivate
```
</details>

## TODO

1. create sr-iov using:

    and pass the nics to the VMs.

2. enable ssd passthrough.
