# lbprox Guide

- [lbprox Guide](#lbprox-guide)
  - [`lbprox` Installation](#lbprox-installation)
  - [Setup workdir for lbprox cli](#setup-workdir-for-lbprox-cli)
  - [Initial Setup](#initial-setup)
    - [Storage setup](#storage-setup)
    - [Open VFs](#open-vfs)
    - [Create Simple Zone for hypervisor internal data network](#create-simple-zone-for-hypervisor-internal-data-network)
  - [Day-To-Day Operations](#day-to-day-operations)
    - [Flavors and Descriptors](#flavors-and-descriptors)
    - [Install Lightbits On VMs](#install-lightbits-on-vms)
    - [Query cluster resources](#query-cluster-resources)
    - [Delete Everything](#delete-everything)
  - [TODO](#todo)

## `lbprox` Installation

`lbprox` is a python package that is recommended to be installed in virtual environment.

In order to setup virtual environment. The following commands will install venv if not
already installed on your system, and will create a virtual-environment under `.venv`
folder. It will also activate this virtual environment, and all installations will
be placed at this env.

```bash
sudo apt install python3-venv
cd lbprox
python3 -m venv .venv
source .venv/bin/activate
```

Now we want to install the `lbprox` package with all it's dependencies. Following
command will install this package:

> `NOTE:`
>
> `-e` state that we install this package in edit mode (for dev purposes) so if we
> change the source code it will apply to the installed command line immediately without
> needing to reinstall the package.

```bash
pip install -e .
```

Now you can run the following command to get the help output:

```bash
lbprox
```

## Setup workdir for lbprox cli

some of the files generated by lbprox will be stored on the base directory at `~/.local/lbprox`

```bash
mkdir -p ~/.local/lbprox
```

Initially you would need to create a config file stating the servers in proxmox cluster that are present.

For example place the following file: `~/.local/lbprox/lbprox.yml`:

```yaml
nodes:
- hostname: rack16-server01
- hostname: rack16-server02
last_active: ""
```

In order to deactivate from this environment just run the following command:

```bash
deactivate
```

## Initial Setup

### Storage setup

First we need to setup local-storage as Directory to hold our VMs and NVMe emulated SSDs.
In the following example we will utilize `/dev/nvme0n1` as a local attached drive that we will
Create a storage on it.

> NOTE:
>
> Today we require that all nodes will have the same block-device (like `/dev/nvme0n1`) in order to create
> a unified storage on each server in the cluster.
> The following command will fail in case not all servers have the same block-device
> We should make this more generic in the future.

Wipe the disk and create a new partition with ext4 on it and will add it as a storage named `lb-local-storage`:

```bash
lbprox image-store create rack16-server01 -s lb-local-storage -d /dev/nvme0n1

```

Now we need to pull the lightbits basic image to install:

> NOTE:
>
> By default each node's `lb-local-storage` will be populated with this image.
> You can specify specific target nodes using the multiple --node/-n flag:

```bash
lbprox os-images create -u https://pulp03.lab.lightbitslabs.com/pulp/content/rocky-9-target/qcow2/latest/rocky-9-target.qcow2
```

```bash
lbprox os-images create -u https://pulp03.lab.lightbitslabs.com/pulp/content/ubuntu-24.04-initiator/qcow2/latest/ubuntu-24.04-initiator.qcow2
```

List the os-images with the following command (filtered by iso):

```bash
lbprox os-images list -c iso
```

In order to delete an image from one or all the nodes you can run the following command:

```bash
lbprox os-images delete --volid lb-local-storage:iso/ubuntu-24.04-initiator.img
```

If we want to reverse this process and clean everything up we should run the following:

```bash
lbprox image-store delete rack16-server01 -s lb-local-storage
```

### Open VFs

We would want to open virtual functions on the nics so we can add them as passthrough to the VMs.
On the Proxmox server run the following command to open all VFs on available Mellanox devices:

```bash
lbprox setup-data-network
```

### Create Simple Zone for hypervisor internal data network

In order to provide isolation, and DHCP inside each Proxmox node we can utilize the simple Zone network.

Following command will deploy on all nodes the same DHCP server with the same IPAM configuration that will
allow allocating dynamic IPs in the range: `10.101.1.10-10.101.1.200`

```bash
lbprox --debug data-network create --zone-name=data0
```

> NOTE:
>
> This is a cluster level command that will configure this for every node that exists in the cluster
> This command should be run every time we add a new node to the Proxmox cluster.

In order to delete this network and all other resources related you can run the following command:

```bash
lbprox delete-data-network rack16-server01 --zone-name=data0
```

## Day-To-Day Operations

### Flavors and Descriptors

A machine flavor is a well defined set of machine properties like CPU memory ssds and network configuration.

We provide a set of predefined flavors and one can add flavors as he wish:

By running the following command:

Flavor name should strive to be in the following format:

`<role>-<size>-<numa-count>-<extra_info>`

Example roles:

- target
- initiator
- observability

Example sizes:

- tiny
- small
- large

```bash
lbprox allocation list flavors
+-------+-------------------------------------+-------+-------------+------+--------+------+
| index | name                                | cores | base_memory | ssds | ifaces | numa |
+-------+-------------------------------------+-------+-------------+------+--------+------+
| 1     | single-small-emulated-ssd-target    | 9     | 12GB        | 4    | 2      | 1    |
| 2     | single-small-passthrough-ssd-target | 9     | 12GB        | 4    | 2      | 1    |
| 3     | small-initiator                     | 1     | 1GB         | 0    | 2      | 1    |
| 4     | observability                       | 2     | 4GB         | 0    | 1      | 1    |
+-------+-------------------------------------+-------+-------------+------+--------+------+
```

Or create entire cluster:

first list the allocation-descriptors that exists:

```bash
+-------+-----------------------------------------------------------------+---------------+--------------------------------------+
| index | name                                                            | machine count | description                          |
+-------+-----------------------------------------------------------------+---------------+--------------------------------------+
| 1     | lightbits_cluster_12x_target_small_1numa_emulated_ssd_1x_client | 13            | 12 target cluster with 1 client      |
| 2     | lightbits_cluster_12x_target_tiny_1numa_emulated_ssd_1x_client  | 13            | 12 tiny target cluster with 1 client |
| 3     | lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client   | 4             | 3 tiny targets cluster with 1 client |
| 4     | lightbits_cluster_3x_target_small_1numa_emulated_ssd_1x_client  | 4             | 3 targets cluster with 1 client      |
| 5     | observability                                                   | 1             | all in 1 observability VM            |
+-------+-----------------------------------------------------------------+---------------+--------------------------------------+
```

Running the following command will deploy the VMs described by selected descriptor - `lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client`

```bash
lbprox allocations create rack16-server01 -n lightbits_cluster_3x_target_tiny_1numa_emulated_ssd_1x_client
```

> **NOTE:**
>
> If you want to create a VM with a none emulated SSDs you should first verify that you have enough
> free unattached SSDs on your node and use the `--no-emulated --disk-count=4` option to specify we
> want to use passthrough SSDs

### Install Lightbits On VMs

The following command will install lightbits software (`v3.10.1`) on the pre-allocated VMs.
It will match the tag `cname.c00` and create inventory for every server that is part of this cluster.

The inventory files will be created at: `~/.local/lbprox/inventories/<cluster_id>`

By default the command will install lightbits by running lb-ansible from this location.

```bash
lbprox allocations deploy lightbits -a <allocation_id> --base-url=https://pulp02/pulp/content/releases/lightbits/3.10.1/rhel/9/67/ -p <profile-name>
```

In order to deploy the initiator you can run the following command:

```bash
lbprox allocations deploy nvme-initiator -a <allocation_id> --base-url=https://pulp02/pulp/content/releases/lightbits/3.10.1/rhel/9/67/
```

### Query cluster resources

```bash
lbprox list-cluster-resources <qemu|node|storage> | jq
```

### Delete Everything

Delete VMs one by one:

```bash
lbprox allocations delete -a <allocation_id>
```

## TODO

1. create sr-iov using:
  
    and pass the nics to the VMs.

2. enable ssd passthrough.
